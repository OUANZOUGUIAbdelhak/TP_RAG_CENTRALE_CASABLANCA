# Configuration file for RAG System
# No hardcoded values - everything goes here!

# Paths and directories
paths:
  data_dir: "./data/files"  # where your PDF documents are stored (inside data/files folder)
  vectorstore_dir: "./data/vectorstore"  # where the vector database will be saved (inside data folder)
  chat_sessions_dir: "./data/chat_sessions"  # where chat sessions will be saved (inside data folder)
  
# Embedding model configuration
embedding:
  model_name: "BAAI/bge-large-en-v1.5"  # Better embedding model for LlamaIndex
  # Other options you could try:
  # - "BAAI/bge-base-en-v1.5" (better quality, slower)
  # - "sentence-transformers/all-MiniLM-L6-v2" (faster, smaller)
  # - "sentence-transformers/all-mpnet-base-v2" (best quality, slowest)
  
# Document processing settings
document_processing:
  chunk_size: 1024  # how big each text chunk should be (in characters)
                    # Larger chunks (1024-2048) preserve more context for better answers
                    # Smaller chunks (512-1024) are more focused but may lose context
  chunk_overlap: 128  # how much overlap between chunks (helps maintain context)
                      # Should be ~10-20% of chunk_size for optimal results
  file_type: "pdf"  # what kind of documents we're loading
  
# Retrieval settings (Q2)
retrieval:
  default_k: 10  # default number of documents to retrieve
  score_threshold: 0.5  # only return documents with score below this (lower = better)
  
# LLM settings (for Q3)
llm:
  model_name: "google/flan-t5-base"  # HuggingFace model (fallback if Groq not available)
  temperature: 0.7  # controls randomness (0 = focused, 1 = creative)
  max_tokens: 512  # maximum length of generated response

# Groq LLM settings (for Q3) - Preferred LLM
groq:
  api_key: "your_groq_api_key"  # Your Groq API key
  model: "llama-3.3-70b-versatile"  # Groq model to use
  temperature: 0.7  # controls randomness (0 = focused, 1 = creative)

# Gemini LLM settings (for Q3) - Fallback LLM when Groq is unavailable
gemini:
  api_key: "AIzaSyBJoxKAeUoeT-2wt9VOP_fOPINHXEKzzW8"  # Your Gemini API key
  model: "gemini-2.0-flash"  # Gemini model to use (matches your API setup)
  temperature: 0.7  # controls randomness (0 = focused, 1 = creative)
  
# Evaluation settings (for Q4)
evaluation:
  metrics:
    - "relevance"  # how relevant are the retrieved documents?
    - "accuracy"  # how accurate are the LLM answers?
  
# Chatbot settings (for Q5 - bonus)
chatbot:
  max_history: 5  # how many previous messages to remember
  system_prompt: "You are a helpful assistant that answers questions based on the provided documents."
